- https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html

## Overview
구조화된 스트리밍은 Spark SQL 엔진을 기반으로 구축된 확장 가능하고 내결함성이 뛰어난 스트림 처리 엔진입니다. 정적 데이터에 대한 일괄 계산을 표현하는 것과 동일한 방식으로 스트리밍 계산을 표현할 수 있습니다. Spark SQL 엔진은 스트리밍 데이터가 계속 도착함에 따라 이를 점진적이고 지속적으로 실행하고 최종 결과를 업데이트하는 작업을 처리합니다. Scala, Java, Python 또는 R에서 데이터 세트/데이터 프레임 API를 사용하여 스트리밍 집계, 이벤트 시간 창, 스트림-배치 조인 등을 표현할 수 있습니다. 계산은 최적화된 동일한 Spark SQL 엔진에서 실행됩니다. 마지막으로, 이 시스템은 체크포인트와 미리 쓰기 로그를 통해 엔드 투 엔드 정확히 한 번 내결함성을 보장합니다. 요컨대, 구조화된 스트리밍은 사용자가 스트리밍에 대해 추론할 필요 없이 빠르고 확장 가능하며 내결함성을 갖춘 엔드투엔드 정확히 한 번 스트림 처리를 제공합니다.

내부적으로, 기본적으로 구조화된 스트리밍 쿼리는 데이터 스트림을 일련의 작은 배치 작업으로 처리하는 마이크로 배치 처리 엔진을 사용하여 처리되므로 100밀리초의 낮은 엔드투엔드 지연 시간과 정확한 1회 내결함성을 보장합니다. 하지만 Spark 2.3부터는 연속 처리라는 새로운 저지연 처리 모드를 도입하여 최소 1밀리초의 엔드투엔드 지연 시간을 달성하고 최소 1회 내결함성을 보장할 수 있게 되었습니다. 쿼리에서 데이터 세트/데이터 프레임 작업을 변경하지 않고도 애플리케이션 요구사항에 따라 모드를 선택할 수 있습니다.

이 가이드에서는 프로그래밍 모델과 API에 대해 안내해 드리겠습니다. 기본 마이크로 배치 처리 모델을 중심으로 개념을 설명한 다음 나중에 연속 처리 모델에 대해 설명하겠습니다. 먼저 구조화된 스트리밍 쿼리의 간단한 예제인 스트리밍 단어 수부터 시작하겠습니다.

## Programming Model
구조화된 스트리밍의 핵심 아이디어는 라이브 데이터 스트림을 지속적으로 추가되는 테이블로 취급하는 것입니다. 이는 배치 처리 모델과 매우 유사한 새로운 스트림 처리 모델로 이어집니다. 스트리밍 연산을 정적 테이블에서와 같이 표준 배치와 같은 쿼리로 표현하면, Spark는 이를 무제한 입력 테이블에서 증분 쿼리로 실행합니다. 이 모델을 좀 더 자세히 이해해 보겠습니다.

## Basic Concepts
입력 데이터 스트림을 "입력 테이블"로 간주합니다. 스트림에 도착하는 모든 데이터 항목은 입력 테이블에 새로운 행이 추가되는 것과 같습니다.

https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png

입력에 대한 쿼리는 "결과 테이블"을 생성합니다. 트리거 간격마다(예를 들어, 1초마다) 새 행이 입력 테이블에 추가되고, 이는 결국 결과 테이블을 업데이트합니다. 결과 테이블이 업데이트될 때마다 변경된 결과 행을 외부 싱크에 쓰고 싶을 것입니다.

https://spark.apache.org/docs/latest/img/structured-streaming-model.png

"출력"은 외부 저장소에 기록되는 내용을 정의합니다. 출력은 다른 모드로 정의할 수 있습니다:

- Complete Mode(완료 모드) - 업데이트된 전체 결과 테이블이 외부 저장소에 기록됩니다. 전체 테이블의 쓰기를 처리하는 방법은 스토리지 커넥터가 결정합니다.

- Append Mode(추가 모드) - 마지막 트리거 이후 결과 테이블에 추가된 새 행만 외부 저장소에 기록됩니다. 이 모드는 결과 테이블의 기존 행이 변경되지 않을 것으로 예상되는 쿼리에만 적용됩니다.

- Update Mode(업데이트 모드) - 마지막 트리거 이후 결과 테이블에서 업데이트된 행만 외부 저장소에 기록됩니다(Spark 2.1.1부터 사용 가능). 이 모드는 마지막 트리거 이후 변경된 행만 출력한다는 점에서 전체 모드와 다릅니다. 쿼리에 집계가 포함되지 않은 경우, 추가 모드와 동일합니다.

각 모드는 특정 유형의 쿼리에만 적용된다는 점에 유의하세요. 이에 대해서는 나중에 자세히 설명합니다.

이 모델의 사용법을 설명하기 위해 위의 빠른 예제의 맥락에서 모델을 이해해 보겠습니다. 첫 번째 줄 DataFrame은 입력 테이블이고, 마지막 wordCounts DataFrame은 결과 테이블입니다. 스트리밍 라인에 대한 쿼리 DataFrame에서 wordCounts를 생성하는 것은 정적 데이터 프레임과 완전히 동일하다는 점에 유의하세요. 그러나 이 쿼리가 시작되면 Spark는 소켓 연결에서 새 데이터가 있는지 계속 확인합니다. 새 데이터가 있는 경우, Spark는 아래와 같이 이전에 실행 중인 카운트와 새 데이터를 결합하여 업데이트된 카운트를 계산하는 "증분" 쿼리를 실행합니다.

https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png

구조화된 스트리밍은 전체 테이블을 구체화하지 않는다는 점에 유의하세요. 스트리밍 데이터 소스에서 사용 가능한 최신 데이터를 읽고, 이를 점진적으로 처리하여 결과를 업데이트한 다음 소스 데이터를 버립니다. 결과를 업데이트하는 데 필요한 최소한의 중간 상태 데이터만 유지합니다(예: 앞의 예제에서 중간 카운트).

이 모델은 다른 많은 스트림 처리 엔진과 크게 다릅니다. 많은 스트리밍 시스템에서는 사용자가 직접 실행 중인 집계를 유지 관리해야 하므로 내결함성 및 데이터 일관성(최소 한 번, 최대 한 번 또는 정확히 한 번)에 대해 추론해야 합니다. 이 모델에서는 새로운 데이터가 있을 때 Spark가 결과 테이블을 업데이트하여 사용자가 추론해야 하는 부담을 덜어줍니다. 예를 들어, 이 모델이 이벤트 시간 기반 처리와 늦게 도착하는 데이터를 어떻게 처리하는지 살펴보겠습니다.

## Handling Event-time and Late Data
이벤트 시간은 데이터 자체에 포함된 시간입니다. 많은 애플리케이션에서 이 이벤트 시간을 기준으로 작업을 수행하고자 할 수 있습니다. 예를 들어, IoT 장치에서 매분 생성되는 이벤트의 수를 얻고자 하는 경우, Spark가 이벤트를 수신하는 시간이 아니라 데이터가 생성된 시간(즉, 데이터의 이벤트 시간)을 사용하고 싶을 것입니다. 이 모델에서 이 이벤트 시간은 매우 자연스럽게 표현됩니다. 장치의 각 이벤트는 테이블의 행이고, 이벤트 시간은 행의 열 값입니다. 따라서 창 기반 집계(예: 매분 이벤트 수)는 이벤트 시간 열에 대한 특수한 유형의 그룹화 및 집계일 뿐이며, 각 시간 창은 하나의 그룹이고 각 행은 여러 창/그룹에 속할 수 있습니다. 따라서 이러한 이벤트 시간 창 기반 집계 쿼리는 정적 데이터 세트(예: 수집된 디바이스 이벤트 로그)와 데이터 스트림 모두에서 일관되게 정의할 수 있으므로 사용자의 작업이 훨씬 쉬워집니다.

또한, 이 모델은 이벤트 시간을 기준으로 예상보다 늦게 도착한 데이터도 자연스럽게 처리합니다. Spark는 결과 테이블을 업데이트하기 때문에 늦은 데이터가 있을 때 이전 집계를 업데이트하는 것은 물론, 중간 상태 데이터의 크기를 제한하기 위해 이전 집계를 정리하는 것도 완전히 제어할 수 있습니다. Spark 2.1부터는 사용자가 늦은 데이터의 임계값을 지정하고 엔진이 그에 따라 오래된 상태를 정리할 수 있는 워터마킹을 지원합니다. 이에 대해서는 나중에 창 작업 섹션에서 자세히 설명합니다.

## Fault Tolerance Semantics
엔드 투 엔드 정확한 1회성 시맨틱을 제공하는 것이 구조화된 스트리밍 설계의 핵심 목표 중 하나였습니다. 이를 달성하기 위해 구조화된 스트리밍 소스, 싱크, 실행 엔진이 처리의 정확한 진행 상황을 안정적으로 추적하여 재시작 및/또는 재처리를 통해 모든 종류의 장애를 처리할 수 있도록 설계했습니다. 모든 스트리밍 소스에는 스트림의 읽기 위치를 추적하기 위한 오프셋(카프카 오프셋 또는 키네시스 시퀀스 번호와 유사)이 있는 것으로 가정합니다. 엔진은 체크포인트와 미리 쓰기 로그를 사용해 각 트리거에서 처리되는 데이터의 오프셋 범위를 기록합니다. 스트리밍 싱크는 재처리를 처리하는 데 무능력하도록 설계되었습니다. 재생 가능한 소스와 비동성 싱크를 함께 사용하면 구조화된 스트리밍은 어떤 장애가 발생하더라도 엔드투엔드의 정확한 1회성 시맨틱을 보장할 수 있습니다.

## API using Datasets and DataFrames
Spark 2.0부터 데이터프레임과 데이터셋은 정적, 제한이 있는 데이터뿐만 아니라 스트리밍, 제한이 없는 데이터도 표현할 수 있습니다. 정적 데이터세트/데이터프레임과 마찬가지로, 공통 진입점인 SparkSession(Scala/Java/Python/R 문서)을 사용하여 스트리밍 소스에서 스트리밍 데이터프레임/데이터세트를 생성하고 정적 데이터프레임/데이터세트와 동일한 연산을 적용할 수 있습니다. 데이터세트/데이터프레임에 익숙하지 않다면 데이터프레임/데이터세트 프로그래밍 가이드를 통해 익숙해지는 것을 적극 권장합니다.

## Creating streaming DataFrames and streaming Datasets
스트리밍 데이터프레임은 SparkSession.readStream()이 반환하는 DataStreamReader 인터페이스(Scala/Java/Python 문서)를 통해 만들 수 있습니다. R에서는 read.stream() 메서드를 사용합니다. 정적 데이터 프레임을 생성하기 위한 읽기 인터페이스와 유사하게, 데이터 형식, 스키마, 옵션 등 소스의 세부 사항을 지정할 수 있습니다.

### Input Sources
몇 가지 기본 제공 소스가 있습니다.

- 파일 소스 - 디렉터리에 기록된 파일을 데이터 스트림으로 읽습니다. 파일은 파일 수정 시간 순서대로 처리됩니다. 최신순으로 설정하면 순서가 뒤바뀝니다. 지원되는 파일 형식은 텍스트, CSV, JSON, ORC, Parquet입니다. 각 파일 형식에 대한 최신 목록과 지원되는 옵션은 DataStreamReader 인터페이스의 문서를 참조하세요. 파일은 지정된 디렉토리에 원자 단위로 배치되어야 하며, 대부분의 파일 시스템에서 파일 이동 작업을 통해 이를 수행할 수 있습니다.

- 카프카 소스 - 카프카에서 데이터를 읽습니다. Kafka 브로커 버전 0.10.0 이상과 호환됩니다. 자세한 내용은 Kafka 연동 가이드를 참조하세요.

- 소켓 소스(테스트용) - 소켓 연결에서 UTF8 텍스트 데이터를 읽습니다. 수신 서버 소켓은 드라이버에 있습니다. 이것은 엔드 투 엔드 내결함성을 보장하지 않으므로 테스트용으로만 사용해야 합니다.

- 속도 소스(테스트용) - 초당 지정된 행 수로 데이터를 생성하며, 각 출력 행에는 타임스탬프와 값이 포함됩니다. 여기서 타임스탬프는 메시지 발송 시간을 포함하는 타임스탬프 유형이고, 값은 첫 번째 행부터 0부터 시작하여 메시지 수를 포함하는 긴 유형입니다. 이 소스는 테스트 및 벤치마킹을 위한 것입니다.

- 마이크로 배치당 속도 소스(테스트용) - 마이크로 배치당 지정된 수의 행으로 데이터를 생성하며, 각 출력 행에는 타임스탬프와 값이 포함됩니다. 여기서 타임스탬프는 메시지 발송 시간을 포함하는 타임스탬프 유형이고, 값은 첫 번째 행부터 0부터 시작하여 메시지 수를 포함하는 긴 유형입니다. 이 데이터 소스는 속도 데이터 소스와 달리 쿼리 실행(트리거 구성, 쿼리 지연 등)에 관계없이 마이크로 배치당 일관된 입력 행 세트를 제공합니다(예: 배치 0은 0~999, 배치 1은 1000~1999 등). 생성된 시간도 마찬가지입니다. 이 소스는 테스트 및 벤치마킹을 위한 것입니다.

일부 소스는 장애 발생 후 체크포인트 오프셋을 사용하여 데이터를 재생할 수 있다는 것을 보장하지 않기 때문에 내결함성이 없습니다. 내결함성 의미에 대한 이전 섹션을 참조하세요. 다음은 Spark의 모든 소스에 대한 세부 정보입니다.

