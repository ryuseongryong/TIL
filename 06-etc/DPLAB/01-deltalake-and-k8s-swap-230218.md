### 시작하기 전에

- 델타레이크(데이터브릭스), 스파크로 유명한 회사
- 데브옵스 + 데이터옵스를 진행하면서 k8s + deltalake 기술을 운영하면서 진행 예정
- 제품 + 기술 홍보 효과를 위해서 진행
- 오픈소스, 기술블로그 등 진행예정이지만 처음으로 시작하는 부분임
- k8s, deltalake
- 학부생들에서 커널 스터디진행, 알아두면 좋을 것이라는 기대감으로 진행했음
- 스터디 주최자, 참여자가 많이 얻어가는 것이 없는 것 같기도 함
- 따라서 지금의 세미나는 도움이 되는 것, 결과가 나올 수 있는 것을 목표로 함
- 앞으로 모임을 어떻게 진행할지, 최근에 회사에서 진행한 내용에 대한 발표

# 발표 내용

## 델타 스토어에 대한 소개

- 처음에는 데이터브릭스에 대한 관심으로 시작, 스파크가 크고 무거운데 잘 개발되어 왔기 때문에 관심, 토벌즈의 카리스마와 운영 능력, 수천만줄의 코드인 리눅스 커널의 성공에 대한 인식
- 구글 맵리듀스, 빅테이블, 구글 파일시스템을 공개하고 얼마 뒤, 
하둡, HDFS, HDS(구글 빅데이터)이 탄생
하둡API로 처리할 수 없으니까, 하이브라이드
너무 느리니까 메모리로 처리하기 위해 스파크
페이퍼에 나왔을 때, 중간 계산 결과를 메모리에 저장하는 것은 당연히 빠른 거 아니냐라고 했는데 지금의 하둡 로그처리는 그렇게 되어 있음. 사실 기술의 성공에는 복잡한 기술보다는 다른 무언가가 필요함
- 오픈소스 기반 창업회사는 많지만 데이터 브릭스는 독보적, 새로운 기술이 나왔을 때 그것이 다가 아니라 기술이 널리 쓰이다가 아쉬운 점이 쌓이면서 새로운 것이 나오고 기존의 것은 레거시가 되고, 또 새로운 기술이 탄생하는 것이 반복됨
- 도커 처음 나왔을 때, 컨테이너라는 기술은 많이 사용되었음, 서버쪽 가상머신, 버추얼토?는 openVC, Containerized한 것이 많이 사용되었음 서버에서 사용하기엔 한계가 있었음. 도커는 처음 나왔을 때 사용성이 혁신적이었음. desktop개발 등에서도 많이 사용됨
이후 구글에서 쿠버네티스를 만들면서, docker container orchestration을 다 제압하고 쿠버네티스가 룰이 되었음. 쿠버네티스는 핵심 기술 중 하나일 뿐이고, 쿠버네티스는 핵심 기술이고 다른 필요한 서비스들이 많이 있음
- 데이터 옵스를 한 지 오래되지는 않았지만, 트렌드가 변화하고 있는 것 같음. 링크드인에서 많은 사람들이 좀 더 모던한 데이터 플랫폼에 대한 니즈가 있음. 델타레이크, 아이스웤, 데이터 카탈로그, lineage(계보), 과도기 상태이다.
- 델타레이크를 알리면서 회사, 개인에 대한 선순환을 일으키려는 의도
- 델타레이크(델타쉐어링)
    - 스냅샷 관리
    - 접근 제어
    - 기술이 단순한데, 그럴 수록 활용도, 수정에 여지가 많아서 좋음
    - 빅데이터는 기본 전제 조건이 mysql, mongodb가 처리할 수 없는 양의 데이터를 처리하기 위해서임. 빅데이터는 기존 데이터베이스에 넣을 수 없었고, 하둡에 json/parquet형태로 넣고 있었는데, 이 때문에 트랜잭션이 안되는 것이 가장 큰 문제
    - 트랜잭션이 뭐냐면, 데이터라이터가 데이터를 쓸 때 분산파일시스템에 저장하는데, 파일을 파티션을 나눠서 쓰게 되는데, A파티션에 B파티션에 쓰게 되지만, 읽는 쪽에서는 모두 읽혀지거나 하나만 읽혀지거나 하는 등의 문제가 있음
    이 부분은 사용하는 상황에 따라서 문제가 될 수도, 안 될 수도 있음(라인에서는 임시파일로 트랜잭션을 어거지로 만들어서 사용함)
    쓸 때와 읽을 때의 상황공유가 되지 않는다는 뜻
    - 데이터는 로그파일을 쓰면서 의미가 있어지는 것. 쓴다고해서 바로 읽을 수는 없음. 실제로 읽는 것은 로그에 반영이 된 것들만 읽을 수 있음 한 번의 작업(얼만큼의 파일을 만들든 간에) 당 로그 파일 하나가 생겨남. 파일 하나(로그)를 쓰는 것은 무조건 트랜잭션 함. 때문에 트랜잭션이 보장되는 것. 이 동작원리는 일반적인 파일시스템에서 하는 것이지만, 모든 서비스에서 사용되는 것임(엘라스틱서치 등, 일반 파일시스템, 데이터베이스에서 사용되는 원리와 동일함)
    - 모든 DB는 최적화를 진행하는데, 델타레이크의 최적화에는 문제가 있었음. 파티션에 있는 파일 자체를 읽는 것은 위험함(상태가 변경되었을 수 있기 때문). 따라서 로그를 읽어야 하는데, 로그파일을 다 읽고, 스냅샷을 만들어야 하는데 모든 로그파일을 읽을 수 없기 때문에 주기적으로 체크포인트를 만듦 때문에 마지막 체크포인트 + 이후 로그만 읽으면 최종 상태를 확인할 수 있음
    - 델타 쉐어링이 가장 관심이 가는 것 중 하나
    
    빅데이터 1.0 빅데이터 수집, 보관, 처리 자체가 목적
    빅데이터 2.0 데이터 관리 측면, 공유/관리, 데이터 거버넌스 등
    Azure MS 데이터 클린 룸에서 구인, 해당부분이 델타 쉐어링
        - 데이터 Reader가 요청, 토큰을 함께 보내줌(인증토큰)
        - 델타쉐어링이 테이블, 파티션, 컬럼에 대한 권한이 있는지를 확인해주고, 데이터를 가져가는 것
        - 공유, 접근 제어에 대한 것은 델타쉐어링이 핵심
- 모임 이름 : 쿠버네티스 기반의 데이터 플랫폼, 데이터 플랫폼 연구소
- 빅데이터 1.0 빅데이터 수집, 보관, 처리 자체가 목적
    - 빅데이터의 효용성에 대한 검증(인공지능 등)
- 빅데이터 2.0 데이터 관리 측면, 공유/관리, 데이터 거버넌스 등

## 어떻게 진행할지

- 업무/취미
- 문제확인
    - 노션/슬랙
- 문제분석
    - 노션/슬랙
- 문제해결
    - 오픈소스 활용(대부분의 세미나)
    - 오픈소스 기여
    - 오픈소스 개발
- 문제공유
    - 기술블로그(medium, velog)
    - 공개세미나

## Swap on K8S

- kubernetes하면서 kernel을 사용할 일이 많음
- 기본적으로 swap이 disabled되어 있음
- kubernetes에서도 기본적으로 막혀있었으나 풀렸음
- 빅데이터와 인공지능을 위한 컴퓨팅 환경
    - 물리 메모리의 크기와 속도가 중요해짐
        - 데이터의 크기가 커질수록 스토리지와 네트워크의 느린 속도 문제가 심각해짐
        - 이를 해결하기 위해 적극적인 메모리 활용(Cache, CPU - L2 cache, disk 등)
    - 제한적인 컴퓨팅 자원 발전속도
        - 프로세서는 압축 가능한 자원 = 관리가 쉬움, 나눠서 쓸 수 있다는 뜻
        - 메모리는 압축 불가능한 자원 = 관리가 어려움
            - 할당은 쉽지만 해제는 어려움(unevictable, dirtypageout, swapout)
- 빅데이터와 인공지능을 위한 소프트웨어
    - 빅데이터 관련 주요 오픈소스(Spark, Trino, Kafka, Elasitcsearch) 에서 메모리를 많이 사용함
    - 정확한 워킹셋 예측 어려움(정확히 얼마나 쓰는 지를 모르기 때문에 여유있게 잡아야 함) → 메모리 활용률 저하 → 구축/운영 비용 증가
- 페이지 캐시([버퍼 캐시)](https://www.notion.so/469c182c46b142eeb116711e78af934a?pvs=21)
    - 페이지 캐시 = 파일의 데이터를 메모리에 보관
    - 버퍼 캐시 = 디렉토리 파일 목록, 아이노드 등을 메모리에 보관
    - 수정되지 않은 페이지 캐시는 바로 반환 가능, 수정된 페이지 캐시는 반영 후 반환 가능
- 익명 페이지
    - 일반적으로 힙 영역을 의미(JVM Heap 포함, JVM Heap이 프로세스 위에서 동작함)
    - 스왑이 없으면 반환 불가능, 스왑이 있으면 반영(Writeback) 후 반환 가능
- 커널 메모리
    - 커널 스택, 페이지 테이블/디렉토리, 슬랩(slab)
- 쿠버네티스가 swap를 사용하지 않게되면, Spark에서 반환이 불가능함. 전체  5기가 중, 2기가는 반환해도 문제가 발생하지 않을 수 있으나, 반환이 불가능하기 때문에 비효율적임
- kafka → spark streamings에서 1분에 1번 저장(마이크로 배치 형태)
- 제연급 서버에(256GB) Spark streamings를 35개 올린 것(request 허용량)
    - 메모리 사용량(RSS)이 증가하다가 뚝 떨어짐(맛탱이 가는 것)
    - free영역이 줄어들다가 한계가 되면, 페이지 캐시까지 같이 끌어서 씀
    - heap만 사용하기 때문에 반환이 안 됨
    - 쿠버네티스 QOS, bestefford는 리밋이 없지만, kill 1순위, k8s에서 각각의 파드, 프로세스가 얼마나 메모리를 사용하는지는 예측할 수 없음
    - 그럼 실제로 256기가 이상을 사용했는지는 알 수 없지만, 워킹셋이 높지 않아서 실제로 다 사용하지 않았을 가능성이 높음
- 제한적인 컴퓨팅 자원의 문제점
    - CGroup(POD)에 충분한 메모리를 할당하지 않았을 떄
        - JVM OOM(-Xmx)
        - CGroup OOM by Kernel(memory.max), CGroup 자체를 OOM
        - 따라서 JVM 5G, + 2G(Heap이 동작하는 프로세스 등 기타)
    - 시스템에 물리 메모리가 부족할 때
        - Process OOM by Kernel
        - K8S PodEviction
            - system memory가 threshold 보다 작아지면, POD kill
            - Pod eviction이 동작했으면 위와 같은 문제가 발생하지 않았음(10s마다, threshold를 크게 잡을 수도 없음. 크게 잡으면 memory가 부족한 것보다는 cache로 활용하지 않는 것이 문제가 됨, 작게 잡으면 threshold에 도달하기 전에 맛탱이가 감)
        - SparkExecutor Timeout = 시스템 과부화
            - SparkExecutor 에서 Hearbeat가 일정 시간 이상 오지 않으면 강제 재시작
        - K8S NodeNotReady = 시스템 과부화
            - 일정시간 이상 노드 상태 확인이 안되면 해당 노드의 모든 Pod 추출
            - kubelet에서 etcd와 heartbeat 같이 확인하는 작업이 있는데, 이 신호까지 전달이 안되면서 꺼지는 현상
    1. 시스템 메모리가 부족하면, OOM으로 프로세스 킬
    2. 시스템 메모리가 부족하면, 페이지 캐시가 반환/할당이 반복, IO가반복되며 CPU, DISKIO가 높아지면서 overhead 과부화
        1. spark → executor에 heartbeat가 전달되지 않아서 죽여버림(정상적으로 동작중이었으나)
- 페이지 캐시를 많이 쓰는 것은 문제가 되지 않을 수 있음(반환 가능)
- 스왑을 이용한 메모리 안정화/최적화
    - K8S 1.22부터 스왑 지원
    - 지속적인 스왑의 문제점 개선
        - 스토리지 성능 개선(HDD → SSD → NVME)
        - 리눅스 커널 스왑 관리 성능 개선
        - 리눅스 커널 캐시 관리 성능 개선(MGLRU)
        - LRU(Locality)에 근거한 코드 개선(어떤 페이지, 익명 페이지를 반환할 것인지에 대한 문제)
        - 스왑은 불확실성에 대한 문제(동작할 수 없는 것을 강제로 동작시키는 것이 시작되는 것, 지연시간이 늘어날 수 있다는 것, 디스크 반환/회수를 반복하는 것)
    - 대부분 워킹셋보다 할당량(RSS)이 훨씬 큼
        - 실제 프로세스가 사용하는 물리 메모리 양(RSS)
        - mmap으로 1기가 할당하면 ondemand로 할당
        - 재접근이 안되는 메모리가 많은데, 실행되다가 루프를 돌고, 데몬에서 사용하지 않는 메모리
        - 워킹셋에서 사용하지 않는 것은 swap out시키면 전체적으로 좋은 것임
- 스왑을 이용한 문제점 개선(메모리 사용량) → 성능 저하 없음(Spark Batch Duration)
    - Swap이 증가하다가 working set에 도달하면서 swap out이 거의 없이 동작하는 상태가 됨
- 스왑을 이용한 문제점 개선(스왑 사용량)
    - 다시 사용하지 않을 메모리를 잘 반환했다는 뜻
    - 때문에 서비스 성능에 큰 영향을 안미치면서 동작했다는 뜻
- 스왑을 이용한 문제점 개선(메모리 사용량)
    - 스왑 + 캐시 사용량이 안정적으로 변화함
- 스왑을 이용한 문제점 개선(스토리지 사용량)
    - 라이트는 발생, 리드는 거의 없음(스왑인이 거의 없었다는 뜻)

### 서버의 변천

1. 서버 1대 1서비스
2. 서버 1대 가상머신 여러대 각각에 서비스
3. 지금은 컨테이너화 함

### 질문들

- 워크로드에 따른 스왑ON/OFF? 쿠버네티스에서 그렇게는 안됨, kubelet node별로 swapbehavior 설정이 됨, limited, unlimited로 설정, limited이면 스왑을 못쓰고, unlimited는 스왑을 사용(QOS bursterble, guaranteed가 스왑을 못씀)
- 스왑은 지연되는 것, 스왑 아웃만 되는 것이면 해제되지 않다면 문제가 될 수 있음
잘쓰고 잘해제가 되었다면, 늘어나지 않았을 텐데?
JAVA RUNTIME ENGINE에서 openJDK로 빌드하고 JAVA code실행하는데 gralVM이 있음, JAVA RUNTIME CODE가 C++이기에, JAVA RUNTIME을 JAVA로 개발하자는 것. JAVA최적화 등을 위해, JVM Heap은 process Heap위에서 동작, JVM Heap은 GC하지만 process Heap에 갖고 있음. 따라서 계속 늘어남. GralVM은 실제로 반환함, 그럼에도 GralVM은 계속 증가하는데 Heap 관리와 관련된 부분, memory를 unlocation/반환할 때 똑같은 순서로 하지 않음. 단편화 때문에 기본적으로 사용량은 올라갈 수 밖에 없음.
10년 전에도 멜롭도 구글이 만든 PTmelop은 단편화, 병렬성, 스레드별로 heap 관리해줌
- SwapOut은 많은데/SwapIn은 없는 것은 워킹셋에 적합한 만큼 동작하는 것(예시는 5G +2G), 스마트팩토리마다 사용량이 다를 수 있는데, 그 부분은?
Spark는 배치 또는 스트리밍으로 사용하는데 batch는 1회성이기 때문에 큰 문제가 없는데, 스트리밍은 데몬으로 동작하기 때문에 고민이 많음(입력되는 데이터 양에 따른 설정이 필요함), 어떤 오퍼레이션을 하는지가 문제, field수를 고객이 마음대로 추가하는 경우, memory 5G는 10ms 2KB/s 크기
spark에서 입력되는 데이터 사이즈가 달라졌을 때, 내부적으로 입려되는 데이터 양에 따라서 partitioning해주는 기능이 있음. 중요하게 쓰는 것은 자세히 알아야 함
- 워킹셋이 새로운 서비스를 사용하는 것은 로칼리티 등을 고려했을 때도 스왑을 켜놓는 것이 나은 것인지? 스왑을 켜놓으면 그래도 느려지더라도 죽지는 않는 것이 나을 수 있음.

### tmpfs

spark이 1분에 1번 미니 배치

kafka, spark이 빅데이터를 많이 다루다 보니, 데이터를 페이지 캐시로 바로 저장하는 형식이 많은데, 파일형태로 저장하고, 다시 지움

배치가 실행될 때, 셔플 내용을 파일시스템에 저장, 1분이 지나면 지움, 자바는 객체가 GC로 지움, GC로 지울 때 연결된 파일도 같이 지움, 좀 있다 지울것이지만 커널을 모르기 때문에 파일에 다시 writeback을 하여 diskIO가 발생(불필요한 IO)

때문에 Spark는 tmpfs(메모리 기반 fs)에 써서 이 부분을 보완함

- nvme 아웃되는 것에 대한 언급

### 문제 확인에 대한 관심

- 실제 의미있는 문제를 해결하고 싶은 것들
- 스왑 사용의 공격적인 패턴, 스왑 사용의 방어적인 패턴
- AWS scale out도 threshold에 친 다음 늘어남